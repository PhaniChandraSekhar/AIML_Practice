{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhaniChandraSekhar/AIML_Practice/blob/main/Week5_Day1_Intro_to_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMIMO0UxiPxW"
      },
      "source": [
        "# Week 5, Day 1: Introduction to Deep Learning\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand neural network fundamentals\n",
        "- Learn about activation functions\n",
        "- Master backpropagation concepts\n",
        "- Practice implementing basic neural networks\n",
        "\n",
        "## Topics Covered\n",
        "1. Neural Network Basics\n",
        "2. Activation Functions\n",
        "3. Forward and Backward Propagation\n",
        "4. Training Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-B7iRdRsiPxZ"
      },
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODr6-A5aiPxa"
      },
      "source": [
        "## 1. Neural Network Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3bDa73aiPxa"
      },
      "source": [
        "def neural_network_basics():\n",
        "    # Generate simple dataset\n",
        "    np.random.seed(42)\n",
        "    X = np.linspace(-10, 10, 100).reshape(-1, 1)\n",
        "    y = np.sin(X) + np.random.normal(0, 0.1, X.shape)\n",
        "\n",
        "    # Create a simple neural network\n",
        "    model = Sequential([\n",
        "        Dense(10, activation='relu', input_shape=(1,)),\n",
        "        Dense(10, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Data and predictions\n",
        "    plt.subplot(121)\n",
        "    plt.scatter(X, y, alpha=0.5, label='Data')\n",
        "    plt.plot(X, y_pred, 'r-', label='Predictions')\n",
        "    plt.title('Neural Network Predictions')\n",
        "    plt.legend()\n",
        "\n",
        "    # Training loss\n",
        "    plt.subplot(122)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nModel Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "neural_network_basics()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbnGhp86iPxb"
      },
      "source": [
        "## 2. Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lERYR1JiPxb"
      },
      "source": [
        "def activation_functions():\n",
        "    # Generate input values\n",
        "    x = np.linspace(-5, 5, 100)\n",
        "\n",
        "    # Define activation functions\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    # Plot activation functions\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # ReLU\n",
        "    plt.subplot(131)\n",
        "    plt.plot(x, relu(x))\n",
        "    plt.title('ReLU')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Sigmoid\n",
        "    plt.subplot(132)\n",
        "    plt.plot(x, sigmoid(x))\n",
        "    plt.title('Sigmoid')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Tanh\n",
        "    plt.subplot(133)\n",
        "    plt.plot(x, tanh(x))\n",
        "    plt.title('Tanh')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print characteristics\n",
        "    print(\"Activation Function Characteristics:\")\n",
        "    print(\"\\nReLU:\")\n",
        "    print(\"- Output range: [0, âˆž)\")\n",
        "    print(\"- Non-linear\")\n",
        "    print(\"- Helps solve vanishing gradient\")\n",
        "\n",
        "    print(\"\\nSigmoid:\")\n",
        "    print(\"- Output range: (0, 1)\")\n",
        "    print(\"- S-shaped curve\")\n",
        "    print(\"- Used in binary classification\")\n",
        "\n",
        "    print(\"\\nTanh:\")\n",
        "    print(\"- Output range: (-1, 1)\")\n",
        "    print(\"- Zero-centered\")\n",
        "    print(\"- Often better than sigmoid\")\n",
        "\n",
        "activation_functions()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElCPoETiPxb"
      },
      "source": [
        "## 3. Forward and Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OAr578aiPxc"
      },
      "source": [
        "def simple_neural_network():\n",
        "    # Simple 2-layer neural network implementation\n",
        "    class SimpleNN:\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "            self.b1 = np.zeros((1, hidden_size))\n",
        "            self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "            self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "        def forward(self, X):\n",
        "            # Forward propagation\n",
        "            self.z1 = np.dot(X, self.W1) + self.b1\n",
        "            self.a1 = np.maximum(0, self.z1)  # ReLU\n",
        "            self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "            self.a2 = self.z2  # Linear output\n",
        "            return self.a2\n",
        "\n",
        "        def backward(self, X, y, learning_rate=0.01):\n",
        "            m = X.shape[0]\n",
        "\n",
        "            # Backward propagation\n",
        "            dz2 = self.a2 - y\n",
        "            dW2 = np.dot(self.a1.T, dz2) / m\n",
        "            db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "            da1 = np.dot(dz2, self.W2.T)\n",
        "            dz1 = da1 * (self.z1 > 0)  # ReLU derivative\n",
        "            dW1 = np.dot(X.T, dz1) / m\n",
        "            db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "            # Update parameters\n",
        "            self.W2 -= learning_rate * dW2\n",
        "            self.b2 -= learning_rate * db2\n",
        "            self.W1 -= learning_rate * dW1\n",
        "            self.b1 -= learning_rate * db1\n",
        "\n",
        "    # Generate data\n",
        "    X = np.random.randn(100, 2)\n",
        "    y = np.sum(X**2, axis=1, keepdims=True)\n",
        "\n",
        "    # Create and train network\n",
        "    nn = SimpleNN(2, 5, 1)\n",
        "    losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(1000):\n",
        "        # Forward pass\n",
        "        pred = nn.forward(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = np.mean((pred - y)**2)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Backward pass\n",
        "        nn.backward(X, y)\n",
        "\n",
        "    # Plot training progress\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "simple_neural_network()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz8Jj-PkiPxc"
      },
      "source": [
        "## Practical Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P68PpixQiPxc"
      },
      "source": [
        "# Exercise 1: Binary Classification\n",
        "\n",
        "def binary_classification_exercise():\n",
        "    # Generate spiral dataset\n",
        "    def generate_spiral_data(n_points=100, noise=0.5):\n",
        "        points = []\n",
        "        labels = []\n",
        "\n",
        "        for i in range(2):\n",
        "            for j in range(n_points):\n",
        "                angle = j / n_points * 2 * np.pi\n",
        "                radius = j / n_points\n",
        "                point = [\n",
        "                    radius * np.cos(angle + i * np.pi),\n",
        "                    radius * np.sin(angle + i * np.pi)\n",
        "                ]\n",
        "                points.append(point)\n",
        "                labels.append(i)\n",
        "\n",
        "        return np.array(points), np.array(labels)\n",
        "\n",
        "    # Generate data\n",
        "    X, y = generate_spiral_data()\n",
        "\n",
        "    # Plot data\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X[y==0, 0], X[y==0, 1], label='Class 0')\n",
        "    plt.scatter(X[y==1, 0], X[y==1, 1], label='Class 1')\n",
        "    plt.title('Spiral Dataset')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Task: Create a neural network for binary classification\")\n",
        "    print(\"1. Design network architecture\")\n",
        "    print(\"2. Choose appropriate activation functions\")\n",
        "    print(\"3. Train the model\")\n",
        "    print(\"4. Visualize decision boundary\")\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "binary_classification_exercise()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZzkKNfOiPxd"
      },
      "source": [
        "# Exercise 2: Function Approximation\n",
        "\n",
        "def function_approximation_exercise():\n",
        "    # Generate data\n",
        "    X = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
        "    y = 0.2 * X**3 + 0.5 * X**2 - X + 2 + np.random.normal(0, 1, X.shape)\n",
        "\n",
        "    # Plot data\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X, y, alpha=0.5)\n",
        "    plt.title('Complex Function Data')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Task: Create a neural network for function approximation\")\n",
        "    print(\"1. Design network architecture\")\n",
        "    print(\"2. Experiment with different layer sizes\")\n",
        "    print(\"3. Train the model\")\n",
        "    print(\"4. Compare with true function\")\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "function_approximation_exercise()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujaVqUpMiPxd"
      },
      "source": [
        "## MCQ Quiz\n",
        "\n",
        "1. What is a neuron in a neural network?\n",
        "   - a) A data point\n",
        "   - b) A mathematical function\n",
        "   - c) A weight matrix\n",
        "   - d) An activation function\n",
        "\n",
        "2. Which activation function is most commonly used in hidden layers?\n",
        "   - a) Linear\n",
        "   - b) ReLU\n",
        "   - c) Sigmoid\n",
        "   - d) Tanh\n",
        "\n",
        "3. What is backpropagation used for?\n",
        "   - a) Forward pass\n",
        "   - b) Weight initialization\n",
        "   - c) Gradient computation\n",
        "   - d) Data preprocessing\n",
        "\n",
        "4. What is the vanishing gradient problem?\n",
        "   - a) Loss becomes zero\n",
        "   - b) Gradients become too small\n",
        "   - c) Network becomes too large\n",
        "   - d) Learning rate is too high\n",
        "\n",
        "5. Which is NOT a hyperparameter in neural networks?\n",
        "   - a) Learning rate\n",
        "   - b) Number of layers\n",
        "   - c) Gradient values\n",
        "   - d) Batch size\n",
        "\n",
        "6. What does the universal approximation theorem state?\n",
        "   - a) Neural networks can solve any problem\n",
        "   - b) Neural networks can approximate any continuous function\n",
        "   - c) Neural networks always converge\n",
        "   - d) Neural networks are always optimal\n",
        "\n",
        "7. Why is ReLU preferred over sigmoid?\n",
        "   - a) More accurate\n",
        "   - b) Faster computation\n",
        "   - c) Reduces vanishing gradient\n",
        "   - d) Better for classification\n",
        "\n",
        "8. What is weight initialization important for?\n",
        "   - a) Final accuracy\n",
        "   - b) Training speed\n",
        "   - c) Network size\n",
        "   - d) Both a and b\n",
        "\n",
        "9. Which layer typically has no activation function?\n",
        "   - a) Input layer\n",
        "   - b) Hidden layer\n",
        "   - c) Output layer (regression)\n",
        "   - d) All layers need activation\n",
        "\n",
        "10. What is the purpose of bias in neural networks?\n",
        "    - a) Regularization\n",
        "    - b) Shift activation function\n",
        "    - c) Speed up training\n",
        "    - d) Reduce overfitting\n",
        "\n",
        "Answers: 1-b, 2-b, 3-c, 4-b, 5-c, 6-b, 7-c, 8-d, 9-c, 10-b"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}